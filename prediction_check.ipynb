{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57dcb1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pyreadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6e45d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/sethl/OneDrive/Programming Stuff/Modeling Folder/NFL Models/playoff_predictor/model_evaluation/data/train_groups.pkl', 'rb') as f:\n",
    "    train_groups = pickle.load(f)\n",
    "\n",
    "with open('C:/Users/sethl/OneDrive/Programming Stuff/Modeling Folder/NFL Models/playoff_predictor/model_evaluation/data/train_y.pkl', 'rb') as f:\n",
    "    train_y = pickle.load(f)\n",
    "\n",
    "with open('C:/Users/sethl/OneDrive/Programming Stuff/Modeling Folder/NFL Models/playoff_predictor/model_evaluation/data/test_groups.pkl', 'rb') as f:\n",
    "    test_groups = pickle.load(f)\n",
    "\n",
    "with open('C:/Users/sethl/OneDrive/Programming Stuff/Modeling Folder/NFL Models/playoff_predictor/model_evaluation/data/test_y.pkl', 'rb') as f:\n",
    "    test_y = pickle.load(f)\n",
    "\n",
    "with open('C:/Users/sethl/OneDrive/Programming Stuff/Modeling Folder/NFL Models/playoff_predictor/model_evaluation/data/train_X.pkl', 'rb') as f:\n",
    "    train_X = pickle.load(f)\n",
    "\n",
    "    \n",
    "with open('C:/Users/sethl/OneDrive/Programming Stuff/Modeling Folder/NFL Models/playoff_predictor/model_training/data/model_df.pkl', 'rb') as f:\n",
    "    model_df = pickle.load(f)\n",
    "with open('C:/Users/sethl/OneDrive/Programming Stuff/Modeling Folder/NFL Models/playoff_predictor/model_training/data/season_context.pkl', 'rb') as f:\n",
    "    season_context = pickle.load(f)\n",
    "\n",
    "\n",
    "# model_df = pyreadr.read_r('C:/Users/sethl/OneDrive/Programming Stuff/Modeling Folder/NFL Models/playoff_predictor/data_ingestion/data/modeling_df.rds')[None]\n",
    "\n",
    "FullData = pyreadr.read_r('C:/Users/sethl/OneDrive/Programming Stuff/Modeling Folder/NFL Models/playoff_predictor/data_ingestion/data/FullData.rds')[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "935656c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FullData = pyreadr.read_r('C:/Users/sethl/OneDrive/Programming Stuff/Modeling Folder/NFL Models/playoff_predictor/data_ingestion/data/FullData.rds')[None]\n",
    "\n",
    "\n",
    "with open('C:/Users/sethl/OneDrive/Programming Stuff/Modeling Folder/NFL Models/playoff_predictor/model_evaluation/data/best_model.pkl', 'rb') as f:\n",
    "    best_model = pickle.load(f)\n",
    "\n",
    "\n",
    "with open('C:/Users/sethl/OneDrive/Programming Stuff/Modeling Folder/NFL Models/playoff_predictor/model_evaluation/data/full_predictions.pkl', 'rb') as f:\n",
    "    full_predictions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "85988375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 7/7 [00:02<00:00,  3.00it/s] \n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "# Set model version alias\n",
    "model_name = \"playoff_prediction_model\"\n",
    "# model_version_alias = \"legendary-cub-637\"\n",
    "model_version = 7\n",
    "\n",
    "model = mlflow.sklearn.load_model(f\"models:/{model_name}/{model_version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "53505527",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_predictions = model.predict(full_predictions.drop(columns = 'season'))\n",
    "output_predictions = pd.Series(output_predictions, index=full_predictions.index)\n",
    "\n",
    "updated_prediction_set = model_df.copy()\n",
    "updated_prediction_set['predicted_label'] = output_predictions\n",
    "updated_prediction_set = updated_prediction_set[['predicted_label'] + [col for col in updated_prediction_set.columns if col != 'predicted_label']]\n",
    "\n",
    "\n",
    "final_predicitons = pd.merge(FullData, updated_prediction_set, how = 'inner', on = ['WinsPR', 'season', 'OTier', 'DTier', 'OrollTier', 'DrollTier'])\n",
    "final_predicitons = final_predicitons[['predicted_label', 'label'] + [col for col in final_predicitons.columns if col not in ['predicted_label', 'label']]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aec532",
   "metadata": {},
   "source": [
    "# Regression check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "418ab756",
   "metadata": {},
   "outputs": [],
   "source": [
    "FullData = pyreadr.read_r('C:/Users/sethl/OneDrive/Programming Stuff/Modeling Folder/NFL Models/playoff_predictor/data_ingestion/data/FullData.rds')[None]\n",
    "\n",
    "\n",
    "with open('C:/Users/sethl/OneDrive/Programming Stuff/Modeling Folder/NFL Models/playoff_predictor/model_evaluation/data/model_df.pkl', 'rb') as f:\n",
    "    model_df = pickle.load(f)\n",
    "\n",
    "\n",
    "with open('C:/Users/sethl/OneDrive/Programming Stuff/Modeling Folder/NFL Models/playoff_predictor/model_evaluation/data/full_predictions.pkl', 'rb') as f:\n",
    "    full_predictions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8039be21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 7/7 [00:02<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "# Set model version alias\n",
    "model_name = \"playoff_prediction_model_regression\"\n",
    "# model_version_alias = \"legendary-cub-637\"\n",
    "model_version = 4\n",
    "\n",
    "model = mlflow.sklearn.load_model(f\"models:/{model_name}/{model_version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f294414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_predictions = model.predict(full_predictions.drop(columns = 'season'))\n",
    "output_predictions = pd.Series(output_predictions, index=full_predictions.index)\n",
    "\n",
    "updated_prediction_set = model_df.copy()\n",
    "updated_prediction_set['predicted_label'] = output_predictions\n",
    "updated_prediction_set = updated_prediction_set[['predicted_label'] + [col for col in updated_prediction_set.columns if col != 'predicted_label']]\n",
    "\n",
    "\n",
    "final_predicitons = pd.merge(FullData, updated_prediction_set, how = 'inner', on = ['WinsPR', 'season', 'OTier', 'DTier', 'OrollTier', 'DrollTier'])\n",
    "final_predicitons = final_predicitons[['predicted_label', 'label'] + [col for col in final_predicitons.columns if col not in ['predicted_label', 'label']]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c5b80c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sethl\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\sklearn.py:1028: UserWarning: [22:11:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\c_api\\c_api.cc:1427: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  self.get_booster().save_model(fname)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "model.save_model('production_model.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980aaa9f",
   "metadata": {},
   "source": [
    "# Compare to old model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc203683",
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[15:01:25] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:1483: Check failed: learner_model_param_.num_feature >= p_fmat->Info().num_col_ (17 vs. 20) : Number of columns does not match number of features in booster.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXGBoostError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     40\u001b[39m     test_sw = pickle.load(f)\n\u001b[32m     42\u001b[39m dtest = xgb.DMatrix(test_X)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m test_predictions, results = \u001b[43mevaluate_regression_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_sw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mevaluate_regression_model\u001b[39m\u001b[34m(model, X, y, sw, type)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Evaluate a regression model using various metrics.\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Make predictions on the provided set\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m predictions = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Calculate evaluation metrics\u001b[39;00m\n\u001b[32m     15\u001b[39m mse = metrics.mean_squared_error(y, predictions, sample_weight=sw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sethl\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sethl\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:2527\u001b[39m, in \u001b[36mBooster.predict\u001b[39m\u001b[34m(self, data, output_margin, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features, training, iteration_range, strict_shape)\u001b[39m\n\u001b[32m   2525\u001b[39m shape = ctypes.POINTER(c_bst_ulong)()\n\u001b[32m   2526\u001b[39m dims = c_bst_ulong()\n\u001b[32m-> \u001b[39m\u001b[32m2527\u001b[39m \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2528\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterPredictFromDMatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2529\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2530\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2531\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_pystr_to_cstr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2532\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2533\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2534\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2535\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2536\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2537\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _prediction_output(shape, dims, preds, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sethl\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:310\u001b[39m, in \u001b[36m_check_call\u001b[39m\u001b[34m(ret)\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[32m    300\u001b[39m \n\u001b[32m    301\u001b[39m \u001b[33;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m \u001b[33;03m    return value from API calls\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret != \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "\u001b[31mXGBoostError\u001b[39m: [15:01:25] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:1483: Check failed: learner_model_param_.num_feature >= p_fmat->Info().num_col_ (17 vs. 20) : Number of columns does not match number of features in booster."
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pickle\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Load the model from the file\n",
    "bst = xgb.Booster()\n",
    "bst.load_model(\"C:/Users/sethl/OneDrive/R/R files/NFL/Shiny Apps/- Shiny App Updates/Update Files/season_prediction_regression.model\")\n",
    "\n",
    "def evaluate_regression_model(model, X, y, sw=None, type='test'):\n",
    "    \"\"\"Evaluate a regression model using various metrics.\"\"\"\n",
    "    # Make predictions on the provided set\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    mse = metrics.mean_squared_error(y, predictions, sample_weight=sw)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(y, predictions, sample_weight=sw))\n",
    "    mae = metrics.mean_absolute_error(y, predictions, sample_weight=sw)\n",
    "    r2 = metrics.r2_score(y, predictions, sample_weight=sw)\n",
    "    \n",
    "    print(f\"{type} MSE: {mse:.2f}\")\n",
    "    print(f\"{type} RMSE: {rmse:.2f}\")\n",
    "    print(f\"{type} MAE: {mae:.2f}\")\n",
    "    print(f\"{type} R²: {r2:.2f}\")\n",
    "\n",
    "    results = {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "    return predictions, results\n",
    "\n",
    "\n",
    "with open('C:/Users/sethl/OneDrive/Programming Stuff/Modeling Folder/NFL Models/playoff_predictor/model_evaluation/data/test_X.pkl', 'rb') as f:\n",
    "    test_X = pickle.load(f)\n",
    "with open('C:/Users/sethl/OneDrive/Programming Stuff/Modeling Folder/NFL Models/playoff_predictor/model_evaluation/data/test_y.pkl', 'rb') as f:\n",
    "    test_y = pickle.load(f)\n",
    "with open('C:/Users/sethl/OneDrive/Programming Stuff/Modeling Folder/NFL Models/playoff_predictor/model_evaluation/data/test_sw.pkl', 'rb') as f:\n",
    "    test_sw = pickle.load(f)\n",
    "\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "test_predictions, results = evaluate_regression_model(bst, dtest, test_y, test_sw, type='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
